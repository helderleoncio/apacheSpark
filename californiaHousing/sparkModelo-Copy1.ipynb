{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10eb593d-f96c-49d1-86df-b50a2dc4c975",
   "metadata": {},
   "source": [
    "# Modelo Spark Apache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05be792e-426a-455d-84f8-b3cff1f88b56",
   "metadata": {},
   "source": [
    "### Importe o FindSpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e15d14b-9ea4-4de4-9726-adb5a9335a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import findspark\n",
    "# Importar o módúlo findspark de modo à utilizar o  \n",
    "# método .init, responsável por inicializar o spark.\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c41dd8-85ea-4857-839e-1e0031e55f8c",
   "metadata": {},
   "source": [
    "### Importe SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5db4a45-dd80-4c40-94e3-cc8872844c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession\n",
    "# Após o spark ter sido encontrado no sistema, é necessário criar \n",
    "# a Sessão Spark, onde é possível configurar os nós do cluster, \n",
    "# bem como a memória alocada para cada um deles.\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ed6b02e-44d7-42d8-9842-ce10a2980c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local[*]\") \\\n",
    "   .appName(\"Linear Regression Model\") \\\n",
    "   .config(\"spark.executor.memory\", \"6gb\") \\\n",
    "   .config('spark.sql.debug.maxToStringFields', 2000) \\\n",
    "   .config('spark.debug.maxToStringFields', 2000) \\\n",
    "   .getOrCreate()\n",
    "   \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ef73a9-71ac-4d89-b959-821b454ec9a8",
   "metadata": {},
   "source": [
    "### Iniciando o Desenvolvimento com Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17003bc-996a-44c2-b337-606a7cce6980",
   "metadata": {},
   "source": [
    "##### Com a sessão spark criada, pode-se trabalhar no ambiente de desenvolvimento. \n",
    "\n",
    "* A primeira etapa é importar o conjunto de dados, neste caso, o arquivo chama-se “Salary_Data.csv”, contendo dados de determinados funcionários, com os seus salários e anos de experiência em determinada função.\n",
    "\n",
    "* Note que os dados foram salvos em uma variável chamada rdd, que significa Resilient Distributed Dataset, a principal estrutura de dados do Spark.\n",
    "  \n",
    "* Essa estrutura permite trabalhar com computação distribuída, ou seja, os dados serão distribuídos entre os nós do cluster, e controlados pelo nó master. Desta forma pode-se processá-los em paralelo, aumentando a velocidade de processamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "327fe958-f5e4-428d-a44f-0cf82aba25dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "rdd = sc.textFile('../datasets/Salary_Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71946052-90d9-447c-af5b-a96556bd38b2",
   "metadata": {},
   "source": [
    "* O método .take é indicado para visualização de uma parcela dos dados. Neste caso, o retorno da função trás 2 entradas do dataset. Note que a primeira entrada é ‘1.1,39343.00’ e a segunda entrada é ‘1.3,46205.00’. Os valores 1.1 e 1.3 representam os anos de experiência de um funcionário, já as entradas 39343.00 e 46205.00 representam os seus respectivos salários."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1607a114-ff51-45db-82d1-6d8190e80a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1.1,39343.00', '1.3,46205.00']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb351fa-f21a-4032-a948-c71ff50e7a32",
   "metadata": {},
   "source": [
    "* Como as entradas vieram juntas na mesma string, é necessário separar os valores pela vírgula. Deste modo, usa-se o método split(“,”), responsável por isso. Também usa-se a função map, que mapeia a operação entre parênteses para todas as linhas do rdd.\n",
    "\n",
    "* Note que foi usado paradigma funcional de programação com a função lambda line: line.split(“,”). O Spark se beneficia deste paradigma, portanto é necessário utilizá-lo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d01bbddb-8b61-4ae5-8a9f-83dbe7da5998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split lines on commas\n",
    "rdd = rdd.map(lambda line: line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61bb1b5b-535e-423c-8b28-62732022b0b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1.1', '39343.00']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the first line\n",
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9436877-890b-4294-a11f-f5e86460f6de",
   "metadata": {},
   "source": [
    "Observe que os valores foram separados pela vírgula, como esperado.\n",
    "\n",
    "Existem os métodos .first e .top, que mostram a primeira linha, e a linha do topo, respectivamente. São métodos semelhantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb5a9420-8167-4b8b-bd47-8937e526bd2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.1', '39343.00']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the first line \n",
    "rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce0676f4-d041-410d-95ab-a66827704a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['9.6', '112635.00']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take top elements\n",
    "rdd.top(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f77662-f5dd-4c4b-8af8-083a0bda0bfb",
   "metadata": {},
   "source": [
    "Nesta etapa, importa-se o módulo Row, onde o rdd faz a transformação para linhas do tipo Row. Essa transformação é necessária pois serão tratados os nomes das colunas, como YearsExperience e Salary, representados pela linha 0 e linha 1 respectivamente.\n",
    "\n",
    "Mapeou-se todas as linhas para a formatação de colunas especificadas, e chamou-se o método .toDF(), onde é feita a transformação do rdd para DataFrame (semelhante ao DataFrame da biblioteca pandas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3be517db-280c-48bb-a02c-15f16edfaf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules \n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c97b223-a434-4b27-9dab-564240611759",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Map the RDD to a DF\n",
    "df = rdd.map(lambda line: Row(YearsExperience=line[0], Salary=line[1])).toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d3da59-faf8-4cb7-a1ec-bb9c1ed02eca",
   "metadata": {},
   "source": [
    "Usando o método .show, é possível inspecionar como o DataFrame está."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e7bfe7d-532a-4771-9432-cee833d60ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|  Salary|YearsExperience|\n",
      "+--------+---------------+\n",
      "|39343.00|            1.1|\n",
      "|46205.00|            1.3|\n",
      "|37731.00|            1.5|\n",
      "|43525.00|            2.0|\n",
      "|39891.00|            2.2|\n",
      "|56642.00|            2.9|\n",
      "|60150.00|            3.0|\n",
      "|54445.00|            3.2|\n",
      "|64445.00|            3.2|\n",
      "|57189.00|            3.7|\n",
      "|63218.00|            3.9|\n",
      "|55794.00|            4.0|\n",
      "|56957.00|            4.0|\n",
      "|57081.00|            4.1|\n",
      "|61111.00|            4.5|\n",
      "|67938.00|            4.9|\n",
      "|66029.00|            5.1|\n",
      "|83088.00|            5.3|\n",
      "|81363.00|            5.9|\n",
      "|93940.00|            6.0|\n",
      "+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the top 20 rows \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0974097-f135-4df7-b000-9ce334f91895",
   "metadata": {},
   "source": [
    "O método .printSchema mostra algumas informações sobre os tipos de dados presentes nas colunas, conforme linha abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b8d46b3-ed61-47f2-ba28-23e8acd1cc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Salary: string (nullable = true)\n",
      " |-- YearsExperience: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the data types of all `df` columns\n",
    "# df.dtypes\n",
    "\n",
    "# Print the schema of `df`\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b37b5f1-eecb-468e-9932-f4f56d8044ca",
   "metadata": {},
   "source": [
    "Criou-se uma função chamada convertColumn, que recebe como argumento o dataframe df, os nomes das colunas, e o novo tipo para as quais serão feitos os casts das colunas.\n",
    "\n",
    "Logo após a criação da função, define-se a variável columns, como uma lista contendo os nomes das colunas do df, e aplica-se a função para o dataframe em si, convertendo os valores para FloatType."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e11bd8d-3104-4ff2-a0d0-ba6e3a4b55df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all from `sql.types`\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3835aa7f-074c-499d-9bd2-5572a6f19378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a custom function to convert the data type of DataFrame columns\n",
    "def convertColumn(df, names, newType):\n",
    "    for name in names: \n",
    "        df = df.withColumn(name, df[name].cast(newType))\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "594c5416-b611-4e0c-98c1-cad813541bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign all column names to `columns`\n",
    "columns = ['YearsExperience', 'Salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e399795b-9d3f-40da-9acf-47693a3ea1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conver the `df` columns to `FloatType()`\n",
    "df = convertColumn(df, columns, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32df418e-96f7-4452-9e03-d9c001ad69ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+\n",
      "| Salary|YearsExperience|\n",
      "+-------+---------------+\n",
      "|39343.0|            1.1|\n",
      "|46205.0|            1.3|\n",
      "|37731.0|            1.5|\n",
      "|43525.0|            2.0|\n",
      "|39891.0|            2.2|\n",
      "|56642.0|            2.9|\n",
      "|60150.0|            3.0|\n",
      "|54445.0|            3.2|\n",
      "|64445.0|            3.2|\n",
      "|57189.0|            3.7|\n",
      "|63218.0|            3.9|\n",
      "|55794.0|            4.0|\n",
      "|56957.0|            4.0|\n",
      "|57081.0|            4.1|\n",
      "|61111.0|            4.5|\n",
      "|67938.0|            4.9|\n",
      "|66029.0|            5.1|\n",
      "|83088.0|            5.3|\n",
      "|81363.0|            5.9|\n",
      "|93940.0|            6.0|\n",
      "+-------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ca681b-3066-4c71-9163-ae49d5f316c4",
   "metadata": {},
   "source": [
    "Também é possível mostrar apenas uma coluna com o método .select."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df18cd57-6d91-4a4d-b199-d925cc2555c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| Salary|\n",
      "+-------+\n",
      "|39343.0|\n",
      "|46205.0|\n",
      "|37731.0|\n",
      "|43525.0|\n",
      "|39891.0|\n",
      "|56642.0|\n",
      "|60150.0|\n",
      "|54445.0|\n",
      "|64445.0|\n",
      "|57189.0|\n",
      "+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Salary').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d9d9f1-da7a-4d06-9009-cbfdd98d5a0c",
   "metadata": {},
   "source": [
    "Outra operação bastante conhecida é o groupby, onde pode-se agrupar os dados por um determinado pivô. Neste caso, usa-se a coluna Salary como pivô, efetuando a contagem dos valores e ordenando-os em ordem decrescente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "358ae6d7-daa2-454b-9152-32daf0624529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:==============================================>       (172 + 4) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|  Salary|count|\n",
      "+--------+-----+\n",
      "|122391.0|    1|\n",
      "|121872.0|    1|\n",
      "|116969.0|    1|\n",
      "|113812.0|    1|\n",
      "|112635.0|    1|\n",
      "|109431.0|    1|\n",
      "|105582.0|    1|\n",
      "|101302.0|    1|\n",
      "| 98273.0|    1|\n",
      "| 93940.0|    1|\n",
      "| 91738.0|    1|\n",
      "| 83088.0|    1|\n",
      "| 81363.0|    1|\n",
      "| 67938.0|    1|\n",
      "| 66029.0|    1|\n",
      "| 64445.0|    1|\n",
      "| 63218.0|    1|\n",
      "| 61111.0|    1|\n",
      "| 60150.0|    1|\n",
      "| 57189.0|    1|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Salary\").count().sort(\"Salary\",ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d077e8-f872-4c18-9e34-ad249f2a802b",
   "metadata": {},
   "source": [
    "Por último, temos o método .describe, que faz a descrição do df baseado nas colunas, retornando uma contagem dos elementos, a média, desvio padrão, valores mínimo e máximo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3dc4337f-549a-4fd1-b7c4-11a759467e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|            Salary|   YearsExperience|\n",
      "+-------+------------------+------------------+\n",
      "|  count|                30|                30|\n",
      "|   mean|           76003.0|5.3133333643277485|\n",
      "| stddev|27414.429784582302| 2.837888172228781|\n",
      "|    min|           37731.0|               1.1|\n",
      "|    max|          122391.0|              10.5|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9c8bec-95b2-482b-8443-f8d8123b9feb",
   "metadata": {},
   "source": [
    "Agora é hora de começar a tratar os dados de modo à deixá-los no formato que o algoritmo de Machine Learning espera. Para isso, usa-se o módulo DenseVector. Este DenseVector é uma maneira otimizada de lidar com valores numéricos, acelerando o processamento realizado pelo Spark.\n",
    "Assim, mapeia-se as linhas do df transformando-as em DenseVector, e cria-se um novo dataframe, chamado df, com as colunas ‘label’ e ‘features’.\n",
    "\n",
    "Recordando que uma Regressão Linear é um problema de Aprendizado Supervisionado, ou seja, o algoritmo necessita do ‘ground truth’, os rótulos das entradas, de modo que ele possa comparar com sua saída e calcular alguma métrica de erro, como Erro Quadrático Médio (do inglês, Mean Squared Error, MSE), bastante empregado em problemas de Regressão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "389763e8-b90a-41aa-bf29-102204288ae0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`medianHouseValue`' given input columns: [Salary, YearsExperience];;\\n'Project [Salary#14, YearsExperience#11, ('medianHouseValue / 100000) AS medianHouseValue#174]\\n+- Project [cast(Salary#0 as float) AS Salary#14, YearsExperience#11]\\n   +- Project [Salary#0, cast(YearsExperience#1 as float) AS YearsExperience#11]\\n      +- LogicalRDD [Salary#0, YearsExperience#1], false\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o160.withColumn.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`medianHouseValue`' given input columns: [Salary, YearsExperience];;\n'Project [Salary#14, YearsExperience#11, ('medianHouseValue / 100000) AS medianHouseValue#174]\n+- Project [cast(Salary#0 as float) AS Salary#14, YearsExperience#11]\n   +- Project [Salary#0, cast(YearsExperience#1 as float) AS YearsExperience#11]\n      +- LogicalRDD [Salary#0, YearsExperience#1], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:113)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:110)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:110)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:88)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:88)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3411)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\n\tat org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2258)\n\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2225)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3875/2400086872.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Adjust the values of `medianHouseValue`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"medianHouseValue\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"medianHouseValue\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Show the first 2 lines of `df`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   1996\u001b[0m         \"\"\"\n\u001b[1;32m   1997\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`medianHouseValue`' given input columns: [Salary, YearsExperience];;\\n'Project [Salary#14, YearsExperience#11, ('medianHouseValue / 100000) AS medianHouseValue#174]\\n+- Project [cast(Salary#0 as float) AS Salary#14, YearsExperience#11]\\n   +- Project [Salary#0, cast(YearsExperience#1 as float) AS YearsExperience#11]\\n      +- LogicalRDD [Salary#0, YearsExperience#1], false\\n\""
     ]
    }
   ],
   "source": [
    "# Import all from `sql.functions` \n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Adjust the values of `medianHouseValue`\n",
    "df = df.withColumn(\"medianHouseValue\", col(\"medianHouseValue\")/100000)\n",
    "\n",
    "# Show the first 2 lines of `df`\n",
    "#df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4028c5f3-44ff-4bee-8d99-f2eef6d67b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(longitude=-122.2300033569336, latitude=37.880001068115234, housingMedianAge=41.0, totalRooms=880.0, totalBedrooms=129.0, population=322.0, households=126.0, medianIncome=8.325200080871582, medianHouseValue=4.526, roomsPerHousehold=6.984126984126984, populationPerHousehold=2.5555555555555554, bedroomsPerRoom=0.14659090909090908)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import all from `sql.functions` if you haven't yet\n",
    "#from pyspark.sql.functions import *\n",
    "\n",
    "# Divide `totalRooms` by `households`\n",
    "roomsPerHousehold = df.select(col(\"totalRooms\")/col(\"households\"))\n",
    "\n",
    "# Divide `population` by `households`\n",
    "populationPerHousehold = df.select(col(\"population\")/col(\"households\"))\n",
    "\n",
    "# Divide `totalBedRooms` by `totalRooms`\n",
    "bedroomsPerRoom = df.select(col(\"totalBedRooms\")/col(\"totalRooms\"))\n",
    "\n",
    "# Add the new columns to `df`\n",
    "df = df.withColumn(\"roomsPerHousehold\", col(\"totalRooms\")/col(\"households\")) \\\n",
    "   .withColumn(\"populationPerHousehold\", col(\"population\")/col(\"households\")) \\\n",
    "   .withColumn(\"bedroomsPerRoom\", col(\"totalBedRooms\")/col(\"totalRooms\"))\n",
    "   \n",
    "# Inspect the result\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d642f57-3764-4953-9f93-ee16e94e15d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve 'bedroomsperhouse' given input columns: [bedroomsPerRoom, households, housingMedianAge, latitude, longitude, medianHouseValue, medianIncome, population, populationPerHousehold, roomsPerHousehold, totalBedrooms, totalRooms];\n'Project [medianHouseValue#945, latitude#65, longitude#55, totalBedrooms#95, totalRooms#85, 'bedroomsperhouse, 'roomsperhouse, populationperhousehold#972, households#115, housingMedianAge#75, population#105]\n+- Project [longitude#55, latitude#65, housingMedianAge#75, totalRooms#85, totalBedrooms#95, population#105, households#115, medianIncome#125, medianHouseValue#945, roomsPerHousehold#961, populationPerHousehold#972, (cast(totalBedRooms#95 as double) / cast(totalRooms#85 as double)) AS bedroomsPerRoom#984]\n   +- Project [longitude#55, latitude#65, housingMedianAge#75, totalRooms#85, totalBedrooms#95, population#105, households#115, medianIncome#125, medianHouseValue#945, roomsPerHousehold#961, (cast(population#105 as double) / cast(households#115 as double)) AS populationPerHousehold#972]\n      +- Project [longitude#55, latitude#65, housingMedianAge#75, totalRooms#85, totalBedrooms#95, population#105, households#115, medianIncome#125, medianHouseValue#945, (cast(totalRooms#85 as double) / cast(households#115 as double)) AS roomsPerHousehold#961]\n         +- Project [longitude#55, latitude#65, housingMedianAge#75, totalRooms#85, totalBedrooms#95, population#105, households#115, medianIncome#125, (cast(medianHouseValue#135 as double) / cast(100000 as double)) AS medianHouseValue#945]\n            +- Project [longitude#55, latitude#65, housingMedianAge#75, totalRooms#85, totalBedrooms#95, population#105, households#115, medianIncome#125, cast(medianHouseValue#8 as float) AS medianHouseValue#135]\n               +- Project [longitude#55, latitude#65, housingMedianAge#75, totalRooms#85, totalBedrooms#95, population#105, households#115, cast(medianIncome#7 as float) AS medianIncome#125, medianHouseValue#8]\n                  +- Project [longitude#55, latitude#65, housingMedianAge#75, totalRooms#85, totalBedrooms#95, population#105, cast(households#6 as float) AS households#115, medianIncome#7, medianHouseValue#8]\n                     +- Project [longitude#55, latitude#65, housingMedianAge#75, totalRooms#85, totalBedrooms#95, cast(population#5 as float) AS population#105, households#6, medianIncome#7, medianHouseValue#8]\n                        +- Project [longitude#55, latitude#65, housingMedianAge#75, totalRooms#85, cast(totalBedrooms#4 as float) AS totalBedrooms#95, population#5, households#6, medianIncome#7, medianHouseValue#8]\n                           +- Project [longitude#55, latitude#65, housingMedianAge#75, cast(totalRooms#3 as float) AS totalRooms#85, totalBedRooms#4, population#5, households#6, medianIncome#7, medianHouseValue#8]\n                              +- Project [longitude#55, latitude#65, cast(housingMedianAge#2 as float) AS housingMedianAge#75, totalRooms#3, totalBedRooms#4, population#5, households#6, medianIncome#7, medianHouseValue#8]\n                                 +- Project [longitude#55, cast(latitude#1 as float) AS latitude#65, housingMedianAge#2, totalRooms#3, totalBedRooms#4, population#5, households#6, medianIncome#7, medianHouseValue#8]\n                                    +- Project [cast(longitude#0 as float) AS longitude#55, latitude#1, housingMedianAge#2, totalRooms#3, totalBedRooms#4, population#5, households#6, medianIncome#7, medianHouseValue#8]\n                                       +- LogicalRDD [longitude#0, latitude#1, housingMedianAge#2, totalRooms#3, totalBedRooms#4, population#5, households#6, medianIncome#7, medianHouseValue#8], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Re-order and select columns\u001b[39;00m\n\u001b[1;32m      3\u001b[0m COLUMNS \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedianHouseValue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotalBedrooms\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotalRooms\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbedroomsperhouse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroomsperhouse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpopulationperhousehold\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhouseholds\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhousingMedianAge\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpopulation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCOLUMNS\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:1685\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols):\n\u001b[1;32m   1665\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1666\u001b[0m \n\u001b[1;32m   1667\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1683\u001b[0m \u001b[38;5;124;03m    [Row(name='Alice', age=12), Row(name='Bob', age=15)]\u001b[39;00m\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1685\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msql_ctx)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve 'bedroomsperhouse' given input columns: [bedroomsPerRoom, households, housingMedianAge, latitude, longitude, medianHouseValue, medianIncome, population, populationPerHousehold, roomsPerHousehold, totalBedrooms, totalRooms];\n'Project [medianHouseValue#945, latitude#65, longitude#55, totalBedrooms#95, totalRooms#85, 'bedroomsperhouse, 'roomsperhouse, populationperhousehold#972, households#115, housingMedianAge#75, population#105]\n+- Project [longitude#55, latitude#65, housingMedianAge#75, totalRooms#85, totalBedrooms#95, population#105, households#115, medianIncome#125, medianHouseValue#945, roomsPerHousehold#961, populationPerHousehold#972, (cast(totalBedRooms#95 as double) / cast(totalRooms#85 as double)) AS bedroomsPerRoom#984]\n   +- Project [longitude#55, latitude#65, housingMedianAge#75, totalRooms#85, totalBedrooms#95, population#105, households#115, medianIncome#125, medianHouseValue#945, roomsPerHousehold#961, (cast(population#105 as double) / cast(households#115 as double)) AS populationPerHousehold#972]\n      +- Project [longitude#55, latitude#65, housingMedianAge#75, totalRooms#85, totalBedrooms#95, population#105, households#115, medianIncome#125, medianHouseValue#945, (cast(totalRooms#85 as double) / cast(households#115 as double)) AS roomsPerHousehold#961]\n         +- Project [longitude#55, latitude#65, housingMedianAge#75, totalRooms#85, totalBedrooms#95, population#105, households#115, medianIncome#125, (cast(medianHouseValue#135 as double) / cast(100000 as double)) AS medianHouseValue#945]\n            +- Project [longitude#55, latitude#65, housingMedianAge#75, totalRooms#85, totalBedrooms#95, population#105, households#115, medianIncome#125, cast(medianHouseValue#8 as float) AS medianHouseValue#135]\n               +- Project [longitude#55, latitude#65, housingMedianAge#75, totalRooms#85, totalBedrooms#95, population#105, households#115, cast(medianIncome#7 as float) AS medianIncome#125, medianHouseValue#8]\n                  +- Project [longitude#55, latitude#65, housingMedianAge#75, totalRooms#85, totalBedrooms#95, population#105, cast(households#6 as float) AS households#115, medianIncome#7, medianHouseValue#8]\n                     +- Project [longitude#55, latitude#65, housingMedianAge#75, totalRooms#85, totalBedrooms#95, cast(population#5 as float) AS population#105, households#6, medianIncome#7, medianHouseValue#8]\n                        +- Project [longitude#55, latitude#65, housingMedianAge#75, totalRooms#85, cast(totalBedrooms#4 as float) AS totalBedrooms#95, population#5, households#6, medianIncome#7, medianHouseValue#8]\n                           +- Project [longitude#55, latitude#65, housingMedianAge#75, cast(totalRooms#3 as float) AS totalRooms#85, totalBedRooms#4, population#5, households#6, medianIncome#7, medianHouseValue#8]\n                              +- Project [longitude#55, latitude#65, cast(housingMedianAge#2 as float) AS housingMedianAge#75, totalRooms#3, totalBedRooms#4, population#5, households#6, medianIncome#7, medianHouseValue#8]\n                                 +- Project [longitude#55, cast(latitude#1 as float) AS latitude#65, housingMedianAge#2, totalRooms#3, totalBedRooms#4, population#5, households#6, medianIncome#7, medianHouseValue#8]\n                                    +- Project [cast(longitude#0 as float) AS longitude#55, latitude#1, housingMedianAge#2, totalRooms#3, totalBedRooms#4, population#5, households#6, medianIncome#7, medianHouseValue#8]\n                                       +- LogicalRDD [longitude#0, latitude#1, housingMedianAge#2, totalRooms#3, totalBedRooms#4, population#5, households#6, medianIncome#7, medianHouseValue#8], false\n"
     ]
    }
   ],
   "source": [
    "# Re-order and select columns\n",
    "\n",
    "COLUMNS = [\"medianHouseValue\", \"latitude\", \"longitude\", \"totalBedrooms\", \"totalRooms\", \"bedroomsperhouse\", \"roomsperhouse\", \"populationperhousehold\",\n",
    "           \"households\", \"housingMedianAge\", \"population\"]\n",
    "          ['medianHouseValue', 'latitude', 'longitude', 'totalBedrooms', 'totalRooms', 'bedroomsperhouse', 'roomsperhouse', 'populationperhousehold', \n",
    "           'households', 'housingMedianAge', 'population']\n",
    "\n",
    "df = df.select(COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a934db-8209-4e74-99d1-212a8767ddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eb6732-23a6-49fb-874e-750b28c8f89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `DenseVector`\n",
    "from pyspark.ml.linalg import DenseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43fe4db-63eb-4191-8681-7ddfa62bcfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the `input_data` \n",
    "input_data = df.rdd.map(lambda x: (x[0], DenseVector(x[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93050903-4eb8-4228-8557-2a59e581cfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"label\",\"features\"]\n",
    "\n",
    "# Replace `df` with the new DataFrame\n",
    "df = spark.createDataFrame(data=input_data, schema = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba65ab36-badd-4e74-8568-95ac0d543861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponha que você tenha um DataFrame chamado df\n",
    "# ...\n",
    "\n",
    "# Acessando os dados da posição 0 usando head() e collect()\n",
    "dados_posicao_0 = df.head(2)[1:]\n",
    "print(dados_posicao_0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
